{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\python3.10\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\python3.10\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\python3.10\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\python3.10\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\python3.10\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_19068\\3291806000.py\", line 5, in <module>\n",
      "    from torchvision import datasets, transforms, models\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\torchvision\\__init__.py\", line 5, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\torchvision\\models\\__init__.py\", line 17, in <module>\n",
      "    from . import detection, optical_flow, quantization, segmentation, video\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\torchvision\\models\\detection\\__init__.py\", line 1, in <module>\n",
      "    from .faster_rcnn import *\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\torchvision\\models\\detection\\faster_rcnn.py\", line 16, in <module>\n",
      "    from .anchor_utils import AnchorGenerator\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py\", line 10, in <module>\n",
      "    class AnchorGenerator(nn.Module):\n",
      "  File \"d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py\", line 63, in AnchorGenerator\n",
      "    device: torch.device = torch.device(\"cpu\"),\n",
      "d:\\DATA SCIENCE\\Deep learning project\\Xray\\env\\lib\\site-packages\\torchvision\\models\\detection\\anchor_utils.py:63: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:77.)\n",
      "  device: torch.device = torch.device(\"cpu\"),\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms, models\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StepLR\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchsummary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'env (Python 3.10.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../artifacts/02_09_2024_13_41_30\\data_ingestion/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\DATA SCIENCE\\\\Deep learning project\\\\Xray'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the class name \n",
    "class_name = ['NORMAL','PNEUMONIA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to  get the list of files\n",
    "def get_list_of_files(dir_name):\n",
    "    '''\n",
    "    input - The input directory location\n",
    "    output - Returns the list the files in the directory\n",
    "    '''\n",
    "    files_list = os.listdir(dir_name)\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path+'/train/'+class_name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_train = get_list_of_files(data_path+'/train/'+class_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'D:\\\\DATA SCIENCE\\\\Deep learning project\\\\Xray\\\\data\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mupload_images_from_folder_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mupload_images_from_folder_structure\u001b[1;34m(base_folder, collection_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupload_images_from_folder_structure\u001b[39m(base_folder, collection_name):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Upload images from a folder structure into MongoDB Atlas.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m        collection_name (str): The collection name where images will be stored.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# Iterate through subfolders (e.g., 'normal', 'abnormal')\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         category_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_folder, category)\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(category_path):\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'D:\\\\DATA SCIENCE\\\\Deep learning project\\\\Xray\\\\data\\train'"
     ]
    }
   ],
   "source": [
    "files_list_pneumonia_train = get_list_of_files(data_path+'/train/'+class_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_pneumonia_train = get_list_of_files(data_path+'/train/'+class_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_test = get_list_of_files(data_path+'/test/'+class_name[0])\n",
    "files_list_pneumonia_test = get_list_of_files(data_path+'/test/'+class_name[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list_normal_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train samples in Normal category {}\".format(len(files_list_normal_train)))\n",
    "print(\"Number of train samples in Pneumonia category {}\".format(len(files_list_pneumonia_train)))\n",
    "print(\"Number of test samples in Normal category {}\".format(len(files_list_normal_test)))\n",
    "print(\"Number of test samples in Pneumonia category {}\".format(len(files_list_pneumonia_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_no = np.random.randint(0,len(files_list_normal_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_path + '/train/NORMAL/'+ files_list_normal_train[rand_img_no]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_path + '/train/PNEUMONIA/'+ files_list_pneumonia_train[np.random.randint(0,len(files_list_pneumonia_train))]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_img_no = np.random.randint(0,len(files_list_normal_test))\n",
    "img = data_path + '/test/NORMAL/'+ files_list_normal_test[rand_img_no]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_path + '/test/PNEUMONIA/'+ files_list_pneumonia_test[np.random.randint(0,len(files_list_pneumonia_test))]\n",
    "print(plt.imread(img).shape)\n",
    "img = mpimg.imread(img)\n",
    "imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                          [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                          [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(os.path.join(data_path, 'train'), transform= train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.ImageFolder(os.path.join(data_path, 'test'), transform= test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size= 2, shuffle= True, pin_memory= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size= 2, shuffle= False, pin_memory= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "print(class_names)\n",
    "print(f'Number of train images: {len(train_data)}')\n",
    "print(f'Number of test images: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creating custom CNN architecture for Image classification\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        # Input Block\n",
    "        self.convolution_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=(3, 3),\n",
    "                      padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8)\n",
    "        )\n",
    "        self.pooling11 = nn.MaxPool2d(2, 2)\n",
    "        # CONVOLUTION BLOCK 1\n",
    "        self.convolution_block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=20, kernel_size=(3, 3),\n",
    "                      padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20)\n",
    "        )\n",
    "        self.pooling22 = nn.MaxPool2d(2, 2)\n",
    "        self.convolution_block3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20, out_channels=10, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10),\n",
    "        )\n",
    "        self.pooling33 = nn.MaxPool2d(2, 2)\n",
    "        # CONVOLUTION BLOCK 2\n",
    "        self.convolution_block4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20)\n",
    "        )\n",
    "        self.convolution_block5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20, out_channels=32, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "#         self.convblock6 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(1, 1), padding='same', bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(10),\n",
    "#         )\n",
    "        self.convolution_block6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10)\n",
    "        )\n",
    "#         self.convblock8 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=10, out_channels=32, kernel_size=(1, 1), padding='same', bias=True),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(32)\n",
    "#         )\n",
    "        self.convolution_block7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(1, 1), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(10)\n",
    "        )\n",
    "        self.convolution_block8 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=14, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(14)\n",
    "        )\n",
    "        self.convolution_block9 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=14, out_channels=16, kernel_size=(3, 3), padding=0, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        # OUTPUT BLOCK\n",
    "        self.gap = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=4)\n",
    "        )\n",
    "        self.convolution_block_out = nn.Sequential(\n",
    "              nn.Conv2d(in_channels=16, out_channels=2, kernel_size=(4, 4), padding=0, bias=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.convolution_block1(x)\n",
    "        x = self.pooling11(x)\n",
    "        x = self.convolution_block2(x)\n",
    "        x = self.pooling22(x)\n",
    "        x = self.convolution_block3(x)\n",
    "        x = self.pooling33(x)\n",
    "        x = self.convolution_block4(x)\n",
    "        x = self.convolution_block5(x)\n",
    "#         x = self.convblock6(x)\n",
    "        x = self.convolution_block6(x)\n",
    "#         x = self.convblock8(x)\n",
    "        x = self.convolution_block7(x)\n",
    "        x = self.convolution_block8(x)\n",
    "        x = self.convolution_block9(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.convolution_block_out(x)\n",
    "        x = x.view(-1, 2)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check weather cuda is available in the system or not \n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Available processor {}\".format(device))\n",
    "model = Net().to(device)\n",
    "# To check the model summary\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Description: To train the model \n",
    "    \n",
    "    input: model,device,train_loader,optimizer,epoch \n",
    "    \n",
    "    output: loss, batch id and accuracy\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader)\n",
    "    correct = 0\n",
    "    processed = 0\n",
    "    for batch_idx, (data, target) in enumerate(pbar):\n",
    "        # get data\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # Initialization of gradient\n",
    "        optimizer.zero_grad()\n",
    "        # In PyTorch, gradient is accumulated over backprop and even though thats used in RNN generally not used in CNN\n",
    "        # or specific requirements\n",
    "        ## prediction on data\n",
    "        y_pred = model(data)\n",
    "        # Calculating loss given the prediction\n",
    "        loss = F.nll_loss(y_pred, target)\n",
    "        train_losses.append(loss)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # get the index of the log-probability corresponding to the max value\n",
    "        pred = y_pred.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        processed += len(data)\n",
    "        pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
    "        train_acc.append(100*correct/processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Description: To test the model\n",
    "    \n",
    "    input: model, device, test_loader\n",
    "    \n",
    "    output: average loss and accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Net().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
    "scheduler = StepLR(optimizer, step_size=6, gamma=0.5)\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH:\", epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    scheduler.step()\n",
    "    print('current Learning Rate: ', optimizer.state_dict()[\"param_groups\"][0][\"lr\"])\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses1 = [float(i.cpu().detach().numpy()) for i in train_losses]\n",
    "train_acc1 = [i for i in train_acc]\n",
    "test_losses1 = [i for i in test_losses]\n",
    "test_acc1 = [i for i in test_acc]\n",
    "fig, axs = plt.subplots(2,2,figsize=(16,10))\n",
    "axs[0, 0].plot(train_losses1)\n",
    "axs[0, 0].set_title(\"Training Loss\")\n",
    "axs[1, 0].plot(train_acc1)\n",
    "axs[1, 0].set_title(\"Training Accuracy\")\n",
    "axs[0, 1].plot(test_losses1)\n",
    "axs[0, 1].set_title(\"Test Loss\")\n",
    "axs[1, 1].plot(test_acc1)\n",
    "axs[1, 1].set_title(\"Test Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
